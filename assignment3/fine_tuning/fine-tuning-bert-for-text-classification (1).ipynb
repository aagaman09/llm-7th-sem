{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30097,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport datetime\nimport gc\nimport random\nfrom nltk.corpus import stopwords\nimport re\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nimport transformers\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup\n","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:03:07.283456Z","iopub.execute_input":"2025-06-28T05:03:07.283817Z","iopub.status.idle":"2025-06-28T05:03:07.289180Z","shell.execute_reply.started":"2025-06-28T05:03:07.283784Z","shell.execute_reply":"2025-06-28T05:03:07.288204Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:03:08.174295Z","iopub.execute_input":"2025-06-28T05:03:08.174599Z","iopub.status.idle":"2025-06-28T05:03:08.265653Z","shell.execute_reply.started":"2025-06-28T05:03:08.174574Z","shell.execute_reply":"2025-06-28T05:03:08.264875Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:03:08.694427Z","iopub.execute_input":"2025-06-28T05:03:08.694796Z","iopub.status.idle":"2025-06-28T05:03:08.768680Z","shell.execute_reply.started":"2025-06-28T05:03:08.694763Z","shell.execute_reply":"2025-06-28T05:03:08.767897Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Data preprocessing\n\n\n* Removing urls from tweet\n* Removing html tags\n* Removing punctuations\n* Removing stopwords\n* Removing emoji","metadata":{"execution":{"iopub.status.busy":"2021-05-29T09:17:08.032003Z","iopub.execute_input":"2021-05-29T09:17:08.032358Z","iopub.status.idle":"2021-05-29T09:17:08.039033Z","shell.execute_reply.started":"2021-05-29T09:17:08.032328Z","shell.execute_reply":"2021-05-29T09:17:08.037761Z"}}},{"cell_type":"code","source":"sw = stopwords.words('english')\n\ndef clean_text(text):\n    \n    text = text.lower()\n    \n    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n\n    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n    \n    html=re.compile(r'<.*?>') \n    \n    text = html.sub(r'',text) #Removing html tags\n    \n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n    for p in punctuations:\n        text = text.replace(p,'') #Removing punctuations\n        \n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    \n    text = \" \".join(text) #removing stopwords\n    \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text) #Removing emojis\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:03:21.683675Z","iopub.execute_input":"2025-06-28T05:03:21.684035Z","iopub.status.idle":"2025-06-28T05:03:21.691588Z","shell.execute_reply.started":"2025-06-28T05:03:21.684003Z","shell.execute_reply":"2025-06-28T05:03:21.690638Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:03:22.409177Z","iopub.execute_input":"2025-06-28T05:03:22.409564Z","iopub.status.idle":"2025-06-28T05:03:22.826724Z","shell.execute_reply.started":"2025-06-28T05:03:22.409521Z","shell.execute_reply":"2025-06-28T05:03:22.825803Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"tweets = df.text.values\nlabels = df.target.values","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:03:24.253743Z","iopub.execute_input":"2025-06-28T05:03:24.254065Z","iopub.status.idle":"2025-06-28T05:03:24.258164Z","shell.execute_reply.started":"2025-06-28T05:03:24.254035Z","shell.execute_reply":"2025-06-28T05:03:24.257251Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### BERT Tokenizer\n\nNext, we need to convert each token to an id as present in the tokenizer vocabulary. If there’s a token that is not present in the vocabulary, the tokenizer will use the special [UNK] token and use its id.\n\n","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:03:27.425821Z","iopub.execute_input":"2025-06-28T05:03:27.426155Z","iopub.status.idle":"2025-06-28T05:03:28.401387Z","shell.execute_reply.started":"2025-06-28T05:03:27.426123Z","shell.execute_reply":"2025-06-28T05:03:28.400422Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed18217a56094ca1b0c570c1d0141ae7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cece8b933407449c985bd89c634a972b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05bf6a21f78d447cbbd36536050f9de5"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"print(' Original: ', tweets[0])\n\nprint('Tokenized: ', tokenizer.tokenize(tweets[0]))\n\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[0])))","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:03:35.435986Z","iopub.execute_input":"2025-06-28T05:03:35.436482Z","iopub.status.idle":"2025-06-28T05:03:35.445783Z","shell.execute_reply.started":"2025-06-28T05:03:35.436441Z","shell.execute_reply":"2025-06-28T05:03:35.444662Z"},"trusted":true},"outputs":[{"name":"stdout","text":" Original:  deeds reason earthquake may allah forgive us\nTokenized:  ['deeds', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'us']\nToken IDs:  [15616, 3114, 8372, 2089, 16455, 9641, 2149]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"max_len = 0\n\nfor sent in tweets:\n\n    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n\n    max_len = max(max_len, len(input_ids))\n\nprint('Max sentence length: ', max_len)","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:03:38.252719Z","iopub.execute_input":"2025-06-28T05:03:38.253019Z","iopub.status.idle":"2025-06-28T05:03:41.322376Z","shell.execute_reply.started":"2025-06-28T05:03:38.252995Z","shell.execute_reply":"2025-06-28T05:03:41.321493Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Max sentence length:  45\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"input_ids = []\nattention_masks = []\n\n# For every tweet...\nfor tweet in tweets:\n \n    encoded_dict = tokenizer.encode_plus(\n                        tweet,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = max_len,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n    \n    input_ids.append(encoded_dict['input_ids'])\n    \n    attention_masks.append(encoded_dict['attention_mask'])\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(labels)\n\nprint('Original: ', tweets[0])\nprint('Token IDs:', input_ids[0])","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:03:43.677232Z","iopub.execute_input":"2025-06-28T05:03:43.677539Z","iopub.status.idle":"2025-06-28T05:03:47.341444Z","shell.execute_reply.started":"2025-06-28T05:03:43.677508Z","shell.execute_reply":"2025-06-28T05:03:47.340519Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"Original:  deeds reason earthquake may allah forgive us\nToken IDs: tensor([  101, 15616,  3114,  8372,  2089, 16455,  9641,  2149,   102,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"#### Train-validation split\n80% of data is split into train and 20% to validation sets.","metadata":{}},{"cell_type":"code","source":"\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset)  - train_size\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:03:47.510309Z","iopub.execute_input":"2025-06-28T05:03:47.510591Z","iopub.status.idle":"2025-06-28T05:03:47.520574Z","shell.execute_reply.started":"2025-06-28T05:03:47.510565Z","shell.execute_reply":"2025-06-28T05:03:47.519708Z"},"trusted":true},"outputs":[{"name":"stdout","text":"6,090 training samples\n1,523 validation samples\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\n#\nbatch_size = 32\n\n\ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:03:48.177722Z","iopub.execute_input":"2025-06-28T05:03:48.178013Z","iopub.status.idle":"2025-06-28T05:03:48.182565Z","shell.execute_reply.started":"2025-06-28T05:03:48.177987Z","shell.execute_reply":"2025-06-28T05:03:48.181705Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels = 2, \n    output_attentions = False, \n    output_hidden_states = False, \n)\n\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:03:50.063149Z","iopub.execute_input":"2025-06-28T05:03:50.063433Z","iopub.status.idle":"2025-06-28T05:04:12.611006Z","shell.execute_reply.started":"2025-06-28T05:03:50.063409Z","shell.execute_reply":"2025-06-28T05:04:12.610225Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"158d67dc94e945eaa0f3786c8d13be6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eadd8651a2a41efa6f628599bd6c94d"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(),\n                  lr = 2e-5,\n                  eps = 1e-8 \n                )","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:04:12.612496Z","iopub.execute_input":"2025-06-28T05:04:12.612899Z","iopub.status.idle":"2025-06-28T05:04:12.618989Z","shell.execute_reply.started":"2025-06-28T05:04:12.612859Z","shell.execute_reply":"2025-06-28T05:04:12.618176Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Fine tuning the model","metadata":{}},{"cell_type":"code","source":"\n\nepochs = 4\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n\ntotal_steps = len(train_dataloader) * epochs\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:04:20.355456Z","iopub.execute_input":"2025-06-28T05:04:20.355805Z","iopub.status.idle":"2025-06-28T05:04:20.359823Z","shell.execute_reply.started":"2025-06-28T05:04:20.355773Z","shell.execute_reply":"2025-06-28T05:04:20.358989Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:04:22.063464Z","iopub.execute_input":"2025-06-28T05:04:22.063847Z","iopub.status.idle":"2025-06-28T05:04:22.068004Z","shell.execute_reply.started":"2025-06-28T05:04:22.063794Z","shell.execute_reply":"2025-06-28T05:04:22.067134Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:04:29.701129Z","iopub.execute_input":"2025-06-28T05:04:29.701447Z","iopub.status.idle":"2025-06-28T05:04:29.705532Z","shell.execute_reply.started":"2025-06-28T05:04:29.701415Z","shell.execute_reply":"2025-06-28T05:04:29.704712Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"seed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\ntraining_stats = []\n\ntotal_t0 = time.time()\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n    t0 = time.time()\n    total_train_loss = 0\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        optimizer.zero_grad()\n        output = model(b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)        \n        loss = output.loss\n        total_train_loss += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n       \n        optimizer.step()\n        scheduler.step()\n\n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    training_time = format_time(time.time() - t0)\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n    \n    print(\"\")\n    print(\"Running Validation...\")\n    t0 = time.time()\n    \n    model.eval()\n    total_eval_accuracy = 0\n    best_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    for batch in validation_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n     \n        with torch.no_grad():        \n            output= model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask,\n                                   labels=b_labels)\n        loss = output.loss\n        total_eval_loss += loss.item()\n        logits = output.logits\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    validation_time = format_time(time.time() - t0)\n    if avg_val_accuracy > best_eval_accuracy:\n        torch.save(model, 'bert_model')\n        best_eval_accuracy = avg_val_accuracy\n\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","metadata":{"execution":{"iopub.status.busy":"2025-06-28T05:05:32.033474Z","iopub.execute_input":"2025-06-28T05:05:32.033805Z","iopub.status.idle":"2025-06-28T05:09:29.129918Z","shell.execute_reply.started":"2025-06-28T05:05:32.033777Z","shell.execute_reply":"2025-06-28T05:09:29.128742Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 4 ========\nTraining...\n\n  Average training loss: 0.47\n  Training epcoh took: 0:00:53\n\nRunning Validation...\n  Accuracy: 0.83\n\n======== Epoch 2 / 4 ========\nTraining...\n\n  Average training loss: 0.36\n  Training epcoh took: 0:00:54\n\nRunning Validation...\n  Accuracy: 0.84\n\n======== Epoch 3 / 4 ========\nTraining...\n\n  Average training loss: 0.29\n  Training epcoh took: 0:00:53\n\nRunning Validation...\n  Accuracy: 0.83\n\n======== Epoch 4 / 4 ========\nTraining...\n\n  Average training loss: 0.25\n  Training epcoh took: 0:00:53\n\nRunning Validation...\n  Accuracy: 0.83\n\nTraining complete!\nTotal training took 0:03:57 (h:mm:ss)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"df_test = pd.read_csv('../input/nlp-getting-started/test.csv')\ndf_test['text'] = df_test['text'].apply(lambda x:clean_text(x))\ntest_tweets = df_test['text'].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T05:18:43.895472Z","iopub.execute_input":"2025-06-28T05:18:43.895828Z","iopub.status.idle":"2025-06-28T05:18:44.095778Z","shell.execute_reply.started":"2025-06-28T05:18:43.895794Z","shell.execute_reply":"2025-06-28T05:18:44.094982Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"model = torch.load('bert_model')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T05:18:59.292384Z","iopub.execute_input":"2025-06-28T05:18:59.292727Z","iopub.status.idle":"2025-06-28T05:18:59.568381Z","shell.execute_reply.started":"2025-06-28T05:18:59.292695Z","shell.execute_reply":"2025-06-28T05:18:59.567701Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"test_input_ids = []\ntest_attention_masks = []\nfor tweet in test_tweets:\n    encoded_dict = tokenizer.encode_plus(\n                        tweet,                     \n                        add_special_tokens = True, \n                        max_length = max_len,           \n                        pad_to_max_length = True,\n                        return_attention_mask = True,\n                        return_tensors = 'pt',\n                   )\n    test_input_ids.append(encoded_dict['input_ids'])\n    test_attention_masks.append(encoded_dict['attention_mask'])\ntest_input_ids = torch.cat(test_input_ids, dim=0)\ntest_attention_masks = torch.cat(test_attention_masks, dim=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T05:19:06.374315Z","iopub.execute_input":"2025-06-28T05:19:06.374621Z","iopub.status.idle":"2025-06-28T05:19:07.920357Z","shell.execute_reply.started":"2025-06-28T05:19:06.374596Z","shell.execute_reply":"2025-06-28T05:19:07.919705Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"test_dataset = TensorDataset(test_input_ids, test_attention_masks)\ntest_dataloader = DataLoader(\n            test_dataset, # The validation samples.\n            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T05:19:14.293071Z","iopub.execute_input":"2025-06-28T05:19:14.293367Z","iopub.status.idle":"2025-06-28T05:19:14.297405Z","shell.execute_reply.started":"2025-06-28T05:19:14.293326Z","shell.execute_reply":"2025-06-28T05:19:14.296480Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"predictions = []\nfor batch in test_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        with torch.no_grad():        \n            output= model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask)\n            logits = output.logits\n            logits = logits.detach().cpu().numpy()\n            pred_flat = np.argmax(logits, axis=1).flatten()\n            \n            predictions.extend(list(pred_flat))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T05:19:20.260607Z","iopub.execute_input":"2025-06-28T05:19:20.260937Z","iopub.status.idle":"2025-06-28T05:19:30.019411Z","shell.execute_reply.started":"2025-06-28T05:19:20.260909Z","shell.execute_reply":"2025-06-28T05:19:30.018674Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"df_output = pd.DataFrame()\ndf_output['id'] = df_test['id']\ndf_output['target'] =predictions\ndf_output.to_csv('submission.csv',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T05:19:30.020683Z","iopub.execute_input":"2025-06-28T05:19:30.020925Z","iopub.status.idle":"2025-06-28T05:19:30.034383Z","shell.execute_reply.started":"2025-06-28T05:19:30.020901Z","shell.execute_reply":"2025-06-28T05:19:30.033719Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}