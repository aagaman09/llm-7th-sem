{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f886e1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T16:51:58.006790Z",
     "iopub.status.busy": "2025-05-08T16:51:58.006524Z",
     "iopub.status.idle": "2025-05-08T16:52:11.896585Z",
     "shell.execute_reply": "2025-05-08T16:52:11.896074Z",
     "shell.execute_reply.started": "2025-05-08T16:51:58.006768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3378fff5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T16:52:16.081036Z",
     "iopub.status.busy": "2025-05-08T16:52:16.080230Z",
     "iopub.status.idle": "2025-05-08T16:52:16.087718Z",
     "shell.execute_reply": "2025-05-08T16:52:16.086875Z",
     "shell.execute_reply.started": "2025-05-08T16:52:16.081006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "       \n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6247cc86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T16:52:18.505591Z",
     "iopub.status.busy": "2025-05-08T16:52:18.505088Z",
     "iopub.status.idle": "2025-05-08T16:52:18.512534Z",
     "shell.execute_reply": "2025-05-08T16:52:18.511817Z",
     "shell.execute_reply.started": "2025-05-08T16:52:18.505571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.Linear((enc_hid_dim) + dec_hid_dim, dec_hid_dim) \n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "       \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        hidden = hidden.squeeze(0).unsqueeze(0).repeat(src_len, 1, 1)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "\n",
    "        return torch.softmax(attention, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529ae807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T16:52:20.366146Z",
     "iopub.status.busy": "2025-05-08T16:52:20.365832Z",
     "iopub.status.idle": "2025-05-08T16:52:20.373309Z",
     "shell.execute_reply": "2025-05-08T16:52:20.372740Z",
     "shell.execute_reply.started": "2025-05-08T16:52:20.366124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim + hid_dim, hid_dim, dropout=dropout) \n",
    "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "       \n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        attention_weights = self.attention(hidden, encoder_outputs)\n",
    "\n",
    "        attention_weights = attention_weights.permute(1, 0)\n",
    "\n",
    "        attention_weights = attention_weights.unsqueeze(1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        weighted_context = torch.bmm(attention_weights, encoder_outputs).squeeze(1)\n",
    "\n",
    "        weighted_context = weighted_context.unsqueeze(0)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, weighted_context), dim=2)\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        \n",
    "\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted_context = weighted_context.squeeze(0) # Squeeze again after unsqueeze(0) for concat\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((output, weighted_context, embedded), dim=1))\n",
    "        \n",
    "        return prediction, hidden, cell, attention_weights.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "549f19ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T16:52:24.025046Z",
     "iopub.status.busy": "2025-05-08T16:52:24.024369Z",
     "iopub.status.idle": "2025-05-08T16:52:24.030879Z",
     "shell.execute_reply": "2025-05-08T16:52:24.030183Z",
     "shell.execute_reply.started": "2025-05-08T16:52:24.025019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hid_dim == decoder.rnn.hidden_size, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "      \n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            # Get output prediction from decoder\n",
    "            output, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "\n",
    "            outputs[t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            input = trg[t, :] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1cdec1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T16:52:49.325331Z",
     "iopub.status.busy": "2025-05-08T16:52:49.324562Z",
     "iopub.status.idle": "2025-05-08T16:52:49.331724Z",
     "shell.execute_reply": "2025-05-08T16:52:49.331162Z",
     "shell.execute_reply.started": "2025-05-08T16:52:49.325302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_cnn_dailymail_dataset(num_train_samples=None, num_val_samples=None):\n",
    "    print(\"Loading CNN/DailyMail dataset...\")\n",
    "    dataset = load_dataset(\"cnn_dailymail\", '2.0.0') \n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "\n",
    "    tokenizer.add_special_tokens({'bos_token': '<sos>', 'eos_token': '<eos>'})\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # Tokenize articles\n",
    "        model_inputs = tokenizer(examples['article'], max_length=512, truncation=True, padding=\"max_length\")\n",
    "        labels = tokenizer(examples['highlights'], max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    if num_train_samples is not None:\n",
    "        tokenized_datasets['train'] = tokenized_datasets['train'].select(range(num_train_samples))\n",
    "        print(f\"Using {len(tokenized_datasets['train'])} training samples.\")\n",
    "    if num_val_samples is not None:\n",
    "        tokenized_datasets['validation'] = tokenized_datasets['validation'].select(range(num_val_samples))\n",
    "        print(f\"Using {len(tokenized_datasets['validation'])} validation samples.\")\n",
    "\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns([\"article\", \"highlights\", \"id\"])\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "    return tokenized_datasets, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d7d21c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T16:53:10.462388Z",
     "iopub.status.busy": "2025-05-08T16:53:10.461704Z",
     "iopub.status.idle": "2025-05-08T16:53:10.467676Z",
     "shell.execute_reply": "2025-05-08T16:53:10.466764Z",
     "shell.execute_reply.started": "2025-05-08T16:53:10.462363Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in tqdm(iterator, desc=\"Training\"):\n",
    "        src = batch['input_ids'].transpose(0, 1).to(model.device) \n",
    "        trg = batch['labels'].transpose(0, 1).to(model.device)  \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[1:].reshape(-1, output_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "862b67d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T16:53:26.950623Z",
     "iopub.status.busy": "2025-05-08T16:53:26.950355Z",
     "iopub.status.idle": "2025-05-08T16:53:26.956528Z",
     "shell.execute_reply": "2025-05-08T16:53:26.955700Z",
     "shell.execute_reply.started": "2025-05-08T16:53:26.950602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator, desc=\"Evaluating\"):\n",
    "            src = batch['input_ids'].transpose(0, 1).to(model.device) \n",
    "            trg = batch['labels'].transpose(0, 1).to(model.device)  \n",
    "\n",
    "            output = model(src, trg, 0) \n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].reshape(-1, output_dim)\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaf2754a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T17:08:40.471958Z",
     "iopub.status.busy": "2025-05-08T17:08:40.471647Z",
     "iopub.status.idle": "2025-05-08T17:08:42.807575Z",
     "shell.execute_reply": "2025-05-08T17:08:42.807044Z",
     "shell.execute_reply.started": "2025-05-08T17:08:40.471936Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading CNN/DailyMail dataset...\n",
      "Tokenizing dataset...\n",
      "Using 300 training samples.\n",
      "Using 50 validation samples.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "NUM_TRAIN = 300\n",
    "NUM_VAL = 50\n",
    "BATCH_SIZE = 4\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1.0\n",
    "\n",
    "tokenized_datasets, tokenizer = load_cnn_dailymail_dataset(num_train_samples=NUM_TRAIN, num_val_samples=NUM_VAL)\n",
    "\n",
    "train_iterator = DataLoader(tokenized_datasets['train'], batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_iterator = DataLoader(tokenized_datasets['validation'], batch_size=BATCH_SIZE)\n",
    "\n",
    "INPUT_DIM = len(tokenizer)\n",
    "OUTPUT_DIM = len(tokenizer)\n",
    "TRG_PAD_IDX = tokenizer.pad_token_id\n",
    "\n",
    "attention = Attention(HID_DIM, HID_DIM) \n",
    "encoder = EncoderRNN(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
    "decoder = DecoderRNN(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT, attention)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5653680f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T17:08:50.579970Z",
     "iopub.status.busy": "2025-05-08T17:08:50.579664Z",
     "iopub.status.idle": "2025-05-08T17:08:50.584278Z",
     "shell.execute_reply": "2025-05-08T17:08:50.583711Z",
     "shell.execute_reply.started": "2025-05-08T17:08:50.579948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7517e32c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T17:08:51.821510Z",
     "iopub.status.busy": "2025-05-08T17:08:51.821053Z",
     "iopub.status.idle": "2025-05-08T17:08:51.827922Z",
     "shell.execute_reply": "2025-05-08T17:08:51.827154Z",
     "shell.execute_reply.started": "2025-05-08T17:08:51.821487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): EncoderRNN(\n",
       "    (embedding): Embedding(30524, 256)\n",
       "    (rnn): LSTM(256, 512, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): DecoderRNN(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(30524, 256)\n",
       "    (rnn): LSTM(768, 512, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=1280, out_features=30524, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e22069f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T17:08:52.425726Z",
     "iopub.status.busy": "2025-05-08T17:08:52.425491Z",
     "iopub.status.idle": "2025-05-08T17:08:52.430321Z",
     "shell.execute_reply": "2025-05-08T17:08:52.429553Z",
     "shell.execute_reply.started": "2025-05-08T17:08:52.425708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX) # Ignore padding index in loss calculation\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a3a95e",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c6ccbe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T17:08:55.538495Z",
     "iopub.status.busy": "2025-05-08T17:08:55.538030Z",
     "iopub.status.idle": "2025-05-08T17:23:04.217325Z",
     "shell.execute_reply": "2025-05-08T17:23:04.216683Z",
     "shell.execute_reply.started": "2025-05-08T17:08:55.538471Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [01:21<00:00,  1.09s/it]\n",
      "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 25s\n",
      "\tTrain Loss: 8.316 | Train PPL: 4088.700\n",
      "\t Val. Loss: 7.734 |  Val. PPL: 2285.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [01:21<00:00,  1.08s/it]\n",
      "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 1m 25s\n",
      "\tTrain Loss: 6.934 | Train PPL: 1026.566\n",
      "\t Val. Loss: 7.873 |  Val. PPL: 2624.589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [01:21<00:00,  1.08s/it]\n",
      "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 1m 25s\n",
      "\tTrain Loss: 6.785 | Train PPL: 884.917\n",
      "\t Val. Loss: 7.924 |  Val. PPL: 2763.940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [01:21<00:00,  1.08s/it]\n",
      "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 1m 25s\n",
      "\tTrain Loss: 6.599 | Train PPL: 734.340\n",
      "\t Val. Loss: 8.082 |  Val. PPL: 3234.263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [01:21<00:00,  1.08s/it]\n",
      "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 1m 25s\n",
      "\tTrain Loss: 6.455 | Train PPL: 635.943\n",
      "\t Val. Loss: 8.223 |  Val. PPL: 3724.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [01:21<00:00,  1.08s/it]\n",
      "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 1m 25s\n",
      "\tTrain Loss: 6.340 | Train PPL: 566.680\n",
      "\t Val. Loss: 8.299 |  Val. PPL: 4021.246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [01:21<00:00,  1.08s/it]\n",
      "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 1m 25s\n",
      "\tTrain Loss: 6.162 | Train PPL: 474.220\n",
      "\t Val. Loss: 8.490 |  Val. PPL: 4867.398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [01:21<00:00,  1.08s/it]\n",
      "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 1m 25s\n",
      "\tTrain Loss: 5.990 | Train PPL: 399.382\n",
      "\t Val. Loss: 8.440 |  Val. PPL: 4629.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [01:21<00:00,  1.08s/it]\n",
      "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 1m 25s\n",
      "\tTrain Loss: 5.768 | Train PPL: 319.902\n",
      "\t Val. Loss: 8.540 |  Val. PPL: 5115.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 75/75 [01:21<00:00,  1.08s/it]\n",
      "Evaluating: 100%|██████████| 13/13 [00:03<00:00,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 1m 25s\n",
      "\tTrain Loss: 5.588 | Train PPL: 267.166\n",
      "\t Val. Loss: 8.712 |  Val. PPL: 6077.669\n",
      "Training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):.3f}')\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_seq2seq_attn_model.pt')\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac06c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
